{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMS-DB: Building Agents with Ollama + Graphiti Integration\n",
    "\n",
    "This notebook demonstrates how to use AMS-DB's integrated Ollama + Graphiti framework to build intelligent agents with persistent memory and local LLM capabilities.\n",
    "\n",
    "## Key Features Covered:\n",
    "- ðŸ¤– **Agent Configuration** - Creating agents with AMS-DB's config system\n",
    "- ðŸ§  **Local LLMs** - Running Ollama models for privacy and cost savings  \n",
    "- ðŸ“Š **High-Speed Database** - Polars-based conversation and knowledge storage\n",
    "- ðŸ•¸ï¸ **Knowledge Graphs** - Graphiti temporal knowledge graphs for memory\n",
    "- ðŸ’¬ **Multi-Agent Conversations** - Generate training data and agent interactions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "1. **Ollama installed and running**: `ollama serve`\n",
    "2. **Neo4j running**: Docker or local installation\n",
    "3. **AMS-DB environment**: Virtual environment with dependencies\n",
    "\n",
    "```bash\n",
    "# Install Ollama models\n",
    "ollama pull phi4:latest          # Main LLM\n",
    "ollama pull gemma3:4b           # Small/fast model  \n",
    "ollama pull nomic-embed-text    # Embedding model\n",
    "\n",
    "# Start Neo4j (Docker example)\n",
    "docker run -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/password neo4j:latest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AMS-DB components\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join('.', '..', '..')))\n",
    "\n",
    "from src.ams_db.core.polars_db import PolarsDBHandler\n",
    "from src.ams_db.core.base_agent_config import AgentConfig\n",
    "from src.ams_db.core.graphiti_pipe import GraphitiRAGFramework\n",
    "from src.ams_db.core.conversation_generator import ConversationGenerator\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… AMS-DB components imported successfully!\")\n",
    "print(\"ðŸ“¦ Available components:\")\n",
    "print(\"   â€¢ PolarsDBHandler - High-speed database operations\")\n",
    "print(\"   â€¢ AgentConfig - Agent configuration management\")\n",
    "print(\"   â€¢ GraphitiRAGFramework - Knowledge graph + Ollama integration\")\n",
    "print(\"   â€¢ ConversationGenerator - Multi-agent conversation system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Part 1: Initialize AMS-DB with Ollama + Graphiti\n",
    "\n",
    "AMS-DB provides a unified framework that combines:\n",
    "- **Polars Database** for high-speed data operations\n",
    "- **Ollama** for local LLM processing (privacy + cost savings)\n",
    "- **Graphiti** for temporal knowledge graphs and memory\n",
    "- **Agent Config System** for complete agent management\n",
    "\n",
    "This integration allows you to build sophisticated agents with persistent memory using local models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AMS-DB components with Ollama configuration\n",
    "print(\"ðŸ”§ Initializing AMS-DB with Ollama + Graphiti...\")\n",
    "\n",
    "# 1. Initialize Polars database for high-speed operations\n",
    "db_handler = PolarsDBHandler(\"ams_agent_database\")\n",
    "print(\"âœ… Polars database initialized\")\n",
    "\n",
    "# 2. Initialize Graphiti RAG Framework with Ollama\n",
    "graphiti_framework = GraphitiRAGFramework(\n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\", \n",
    "    neo4j_password=\"password\",\n",
    "    db_path=\"ams_agent_database\",\n",
    "    ollama_base_url=\"http://localhost:11434/v1\",\n",
    "    llm_model=\"phi4:latest\",           # Main reasoning model\n",
    "    small_model=\"gemma3:4b\",           # Fast response model\n",
    "    embedding_model=\"nomic-embed-text\", # Local embeddings\n",
    "    embedding_dim=768\n",
    ")\n",
    "print(\"âœ… Graphiti RAG Framework with Ollama initialized\")\n",
    "\n",
    "# 3. Initialize conversation generator for multi-agent interactions\n",
    "conversation_generator = ConversationGenerator(db_handler, graphiti_framework)\n",
    "print(\"âœ… Multi-agent conversation generator ready\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ AMS-DB system fully initialized!\")\n",
    "print(\"   â€¢ Database: Polars high-speed backend\")\n",
    "print(\"   â€¢ LLM: Ollama local models\")\n",
    "print(\"   â€¢ Memory: Graphiti knowledge graphs\")\n",
    "print(\"   â€¢ Agent System: Complete config management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Part 2: Create Agents with AMS-DB Configuration System\n",
    "\n",
    "AMS-DB's agent configuration system provides complete control over:\n",
    "- **Prompts** - System prompts, boosters, prime directives\n",
    "- **Modalities** - Vision, speech, LaTeX, embeddings  \n",
    "- **Models** - LLM configs, embedding settings\n",
    "- **Databases** - Knowledge bases, conversation history\n",
    "- **Export/Import** - Save and share agent configurations\n",
    "\n",
    "Let's create a specialized \"Research Assistant\" agent that uses Ollama + Graphiti for persistent memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Research Assistant agent with AMS-DB configuration\n",
    "print(\"ðŸ”¬ Creating Research Assistant Agent...\")\n",
    "\n",
    "# 1. Initialize agent configuration\n",
    "research_agent = AgentConfig(\"research_assistant_001\")\n",
    "\n",
    "# 2. Set comprehensive system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an advanced Research Assistant powered by local Ollama models and Graphiti knowledge graphs.\n",
    "\n",
    "Your capabilities include:\n",
    "ðŸ” Deep research and analysis across multiple domains\n",
    "ðŸ§  Persistent memory through knowledge graph integration  \n",
    "ðŸ“Š Data synthesis and pattern recognition\n",
    "ðŸ¤ Collaborative research with other agents\n",
    "ðŸ“š Continuous learning from every interaction\n",
    "\n",
    "You maintain context across conversations and build knowledge incrementally.\n",
    "Always cite your sources and explain your reasoning process.\n",
    "\"\"\"\n",
    "\n",
    "research_agent.set_prompt(\"llmSystem\", system_prompt.strip())\n",
    "research_agent.set_prompt(\"primeDirective\", \n",
    "    \"Conduct thorough research, maintain accurate knowledge graphs, \"\n",
    "    \"and provide evidence-based insights with full transparency.\")\n",
    "\n",
    "# 3. Configure modalities for multimodal capabilities\n",
    "research_agent.set_modality_flag(\"LLM_SYSTEM_PROMPT_FLAG\", True)\n",
    "research_agent.set_modality_flag(\"AGENT_FLAG\", True)  \n",
    "research_agent.set_modality_flag(\"EMBEDDING_FLAG\", True)  # Enable knowledge search\n",
    "research_agent.set_modality_flag(\"LATEX_FLAG\", True)     # Scientific notation\n",
    "research_agent.set_modality_flag(\"LLAVA_FLAG\", True)     # Vision for charts/diagrams\n",
    "\n",
    "# 4. Configure for Ollama models\n",
    "research_agent.update_config({\n",
    "    \"model_config\": {\n",
    "        \"largeLanguageModel\": {\n",
    "            \"names\": [\"phi4:latest\"],\n",
    "            \"instances\": [\"ollama_primary\"],\n",
    "            \"model_config_template\": {\n",
    "                \"base_url\": \"http://localhost:11434/v1\",\n",
    "                \"api_key\": None,\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 4096\n",
    "            }\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"names\": [\"nomic-embed-text\"],\n",
    "            \"instances\": [\"ollama_embedding\"],\n",
    "            \"model_config_template\": {\n",
    "                \"base_url\": \"http://localhost:11434/v1\",\n",
    "                \"embedding_dim\": 768\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# 5. Add agent to database with Graphiti integration\n",
    "agent_id = graphiti_framework.create_agent(\n",
    "    agent_config=research_agent.get_config(),\n",
    "    agent_name=\"Research Assistant\",\n",
    "    description=\"Advanced research agent with Ollama + Graphiti integration\",\n",
    "    tags=[\"research\", \"ollama\", \"graphiti\", \"multimodal\"]\n",
    ")\n",
    "\n",
    "print(f\"âœ… Research Assistant created: {agent_id}\")\n",
    "print(f\"ðŸ§  System prompt: {len(system_prompt)} characters\")\n",
    "print(f\"ðŸŽ›ï¸ Modalities enabled: {sum(research_agent.get_config()['agent_core']['modalityFlags'].values())}\")\n",
    "print(f\"ðŸ¤– Model: Ollama phi4:latest with local embeddings\")\n",
    "\n",
    "# 6. Load agent into Graphiti framework\n",
    "success = graphiti_framework.load_agent(agent_id)\n",
    "print(f\"ðŸ•¸ï¸ Graphiti integration: {'âœ… Active' if success else 'âŒ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Part 3: Add Knowledge to Agent with Graphiti Integration\n",
    "\n",
    "Now we'll add knowledge to our Research Assistant that will be:\n",
    "1. **Stored in Polars** for high-speed querying\n",
    "2. **Embedded with Ollama** using local nomic-embed-text model\n",
    "3. **Indexed in Graphiti** for temporal knowledge graph capabilities\n",
    "4. **Available for RAG** during conversations and research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add research knowledge to the agent with Graphiti + Ollama integration\n",
    "print(\"ðŸ“– Adding knowledge base to Research Assistant...\")\n",
    "\n",
    "# Sample research knowledge entries\n",
    "research_knowledge = [\n",
    "    {\n",
    "        \"title\": \"Transformer Architecture Overview\",\n",
    "        \"content\": \"\"\"\n",
    "        The Transformer architecture, introduced in 'Attention Is All You Need' (Vaswani et al., 2017),\n",
    "        revolutionized natural language processing through the self-attention mechanism.\n",
    "        \n",
    "        Key components:\n",
    "        - Self-attention layers enable parallel processing\n",
    "        - Positional encoding provides sequence information\n",
    "        - Multi-head attention captures different representation subspaces\n",
    "        - Feed-forward networks process attended information\n",
    "        \n",
    "        Applications: GPT, BERT, T5, and most modern LLMs are based on Transformer variants.\n",
    "        \"\"\",\n",
    "        \"source\": \"AI Research\",\n",
    "        \"tags\": [\"transformer\", \"attention\", \"nlp\", \"architecture\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Knowledge Graph Applications in AI\",\n",
    "        \"content\": \"\"\"\n",
    "        Knowledge graphs represent structured relationships between entities, enabling:\n",
    "        \n",
    "        1. Semantic Understanding: Capture real-world relationships\n",
    "        2. Reasoning: Infer new facts from existing knowledge  \n",
    "        3. Explainability: Provide transparent decision paths\n",
    "        4. Context Preservation: Maintain temporal and causal relationships\n",
    "        \n",
    "        Modern applications:\n",
    "        - RAG systems for enhanced LLM responses\n",
    "        - Recommendation engines with entity relationships\n",
    "        - Drug discovery through biochemical pathway modeling\n",
    "        - Financial risk assessment via entity connections\n",
    "        \"\"\",\n",
    "        \"source\": \"Graph Research\",\n",
    "        \"tags\": [\"knowledge_graph\", \"reasoning\", \"rag\", \"relationships\"]\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Local LLM Benefits with Ollama\",\n",
    "        \"content\": \"\"\"\n",
    "        Running local LLMs with Ollama provides significant advantages:\n",
    "        \n",
    "        Privacy Benefits:\n",
    "        - Complete data control and sovereignty\n",
    "        - No external API calls or data transmission\n",
    "        - GDPR and compliance-friendly deployment\n",
    "        \n",
    "        Cost Benefits:  \n",
    "        - No per-token or API usage fees\n",
    "        - Predictable infrastructure costs\n",
    "        - Scalable without variable pricing\n",
    "        \n",
    "        Performance Benefits:\n",
    "        - Reduced latency for local processing\n",
    "        - No network dependency or rate limits\n",
    "        - Customizable model parameters and fine-tuning\n",
    "        \"\"\",\n",
    "        \"source\": \"Local AI Research\", \n",
    "        \"tags\": [\"ollama\", \"privacy\", \"local_llm\", \"cost_efficiency\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add knowledge with Ollama embeddings to both Polars and Graphiti\n",
    "async def add_knowledge_with_integration():\n",
    "    for i, knowledge in enumerate(research_knowledge, 1):\n",
    "        print(f\"ðŸ“š Adding knowledge {i}/{len(research_knowledge)}: {knowledge['title']}\")\n",
    "        \n",
    "        # 1. Add to Polars database for fast querying\n",
    "        kb_id = db_handler.add_knowledge_document(\n",
    "            agent_id=agent_id,\n",
    "            title=knowledge[\"title\"],\n",
    "            content=knowledge[\"content\"],\n",
    "            content_type=\"text\",\n",
    "            source=knowledge[\"source\"],\n",
    "            tags=knowledge[\"tags\"],\n",
    "            metadata={\n",
    "                \"embedded_with\": \"ollama_nomic_embed_text\",\n",
    "                \"integration\": \"graphiti_rag\",\n",
    "                \"knowledge_type\": \"research\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 2. Add to Graphiti with Ollama embeddings\n",
    "        await graphiti_framework.add_knowledge_with_embedding(\n",
    "            title=knowledge[\"title\"],\n",
    "            content=knowledge[\"content\"],\n",
    "            source=knowledge[\"source\"], \n",
    "            tags=knowledge[\"tags\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Added to Polars (ID: {kb_id}) and Graphiti knowledge graph\")\n",
    "\n",
    "# Run the async knowledge integration\n",
    "await add_knowledge_with_integration()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Knowledge integration complete!\")\n",
    "print(f\"   â€¢ Knowledge entries: {len(research_knowledge)}\")\n",
    "print(f\"   â€¢ Embeddings: Ollama nomic-embed-text (local)\")\n",
    "print(f\"   â€¢ Storage: Polars database + Graphiti graph\")\n",
    "print(f\"   â€¢ Agent: {agent_id} ready for research conversations\")\n",
    "\n",
    "# Show knowledge statistics\n",
    "knowledge_stats = db_handler.search_knowledge_base(agent_id, \"transformer\")\n",
    "print(f\"   â€¢ Test search 'transformer': {knowledge_stats.height} results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Part 4: Conversation with Memory and Multi-Agent Interactions\n",
    "\n",
    "Now let's demonstrate the powerful conversation capabilities:\n",
    "1. **Single-agent conversations** with persistent memory via Graphiti\n",
    "2. **Multi-agent interactions** between different specialized agents\n",
    "3. **Knowledge-enhanced responses** using RAG with local Ollama embeddings\n",
    "4. **Training data generation** for fine-tuning and dataset creation\n",
    "\n",
    "All powered by local Ollama models for privacy and Graphiti for temporal memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate conversation with memory and knowledge integration\n",
    "print(\"ðŸ—£ï¸ Starting conversation with Research Assistant...\")\n",
    "\n",
    "async def demonstrate_conversation_with_memory():\n",
    "    \"\"\"Demonstrate conversation with persistent memory and knowledge integration.\"\"\"\n",
    "    \n",
    "    session_id = f\"research_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    print(f\"ðŸ“ Session ID: {session_id}\")\n",
    "    \n",
    "    # Conversation topics that will trigger knowledge retrieval\n",
    "    conversation_turns = [\n",
    "        {\n",
    "            \"user\": \"Can you explain the key innovations in Transformer architecture?\",\n",
    "            \"context\": \"This should retrieve and use our knowledge about Transformers\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"How do knowledge graphs enhance AI systems?\", \n",
    "            \"context\": \"This should access our knowledge graph research\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"What are the benefits of using local LLMs like Ollama?\",\n",
    "            \"context\": \"This should reference our Ollama benefits knowledge\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"How do these three concepts work together in a research system?\",\n",
    "            \"context\": \"This should synthesize across all previous knowledge and conversation\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, turn in enumerate(conversation_turns, 1):\n",
    "        print(f\"\\n--- Turn {i}/{len(conversation_turns)} ---\")\n",
    "        print(f\"ðŸ‘¤ User: {turn['user']}\")\n",
    "        print(f\"ðŸ’­ Expected: {turn['context']}\")\n",
    "        \n",
    "        # 1. Store user message in conversation history\n",
    "        user_msg_id = db_handler.add_conversation_message(\n",
    "            agent_id=agent_id,\n",
    "            role=\"user\", \n",
    "            content=turn[\"user\"],\n",
    "            session_id=session_id,\n",
    "            metadata={\"turn\": i, \"type\": \"research_query\"}\n",
    "        )\n",
    "        \n",
    "        # 2. Search knowledge base with Ollama embeddings\n",
    "        relevant_knowledge = await graphiti_framework.search_knowledge_with_context(\n",
    "            query=turn[\"user\"],\n",
    "            limit=3,\n",
    "            include_graph_context=True\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ” Found {len(relevant_knowledge)} relevant knowledge entries\")\n",
    "        for j, knowledge in enumerate(relevant_knowledge[:2], 1):\n",
    "            title = knowledge.get('title', f'Knowledge {j}')\n",
    "            print(f\"   {j}. {title}\")\n",
    "        \n",
    "        # 3. Add conversation turn to Graphiti (builds temporal memory)\n",
    "        await graphiti_framework.add_conversation_turn(\n",
    "            user_input=turn[\"user\"],\n",
    "            assistant_response=\"[Research Assistant response with knowledge integration]\",\n",
    "            session_id=session_id,\n",
    "            metadata={\"knowledge_entries\": len(relevant_knowledge)}\n",
    "        )\n",
    "        \n",
    "        # 4. Store assistant response in database\n",
    "        assistant_response = f\"\"\"Based on my knowledge base and previous research, I can provide insights on {turn['user'].lower()}.\n",
    "\n",
    "[In a real implementation, this would be generated by Ollama phi4:latest using the retrieved knowledge and conversation context from Graphiti]\n",
    "\n",
    "Key points from knowledge base:\n",
    "{chr(10).join([f\"â€¢ {k.get('title', 'Knowledge')}\" for k in relevant_knowledge[:3]])}\n",
    "\n",
    "This response maintains context from our previous {i-1} conversation turns and builds upon our growing knowledge graph.\"\"\"\n",
    "        \n",
    "        assistant_msg_id = db_handler.add_conversation_message(\n",
    "            agent_id=agent_id,\n",
    "            role=\"assistant\",\n",
    "            content=assistant_response,\n",
    "            session_id=session_id,\n",
    "            metadata={\"turn\": i, \"knowledge_used\": len(relevant_knowledge)}\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ¤– Research Assistant: {assistant_response[:100]}...\")\n",
    "        print(f\"ðŸ’¾ Stored in conversation history (User: {user_msg_id}, Assistant: {assistant_msg_id})\")\n",
    "\n",
    "# Run the conversation demonstration\n",
    "await demonstrate_conversation_with_memory()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Conversation demonstration complete!\")\n",
    "print(\"âœ… Features demonstrated:\")\n",
    "print(\"   â€¢ Knowledge retrieval with Ollama embeddings\") \n",
    "print(\"   â€¢ Temporal memory through Graphiti integration\")\n",
    "print(\"   â€¢ High-speed conversation storage in Polars\")\n",
    "print(\"   â€¢ Context accumulation across conversation turns\")\n",
    "print(\"   â€¢ RAG-enhanced responses with local models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤ Part 5: Multi-Agent Conversation Generation\n",
    "\n",
    "AMS-DB's conversation generator can create multi-agent interactions for:\n",
    "- **Training Data**: Generate diverse conversation datasets for model fine-tuning\n",
    "- **Agent Testing**: Validate agent personalities and knowledge integration\n",
    "- **Research Simulation**: Model collaborative research scenarios\n",
    "- **Data Export**: Create JSONL datasets for machine learning workflows\n",
    "\n",
    "Let's create additional agents and generate multi-agent conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional agents and generate multi-agent conversations\n",
    "print(\"ðŸ‘¥ Creating additional agents for multi-agent conversations...\")\n",
    "\n",
    "# Create a Technical Expert agent\n",
    "tech_expert = AgentConfig(\"tech_expert_001\")\n",
    "tech_expert.set_prompt(\"llmSystem\", \n",
    "    \"You are a Technical Expert specializing in AI/ML implementation details. \"\n",
    "    \"You focus on practical aspects, code examples, and technical best practices. \"\n",
    "    \"You work well with researchers to bridge theory and implementation.\")\n",
    "tech_expert.set_modality_flag(\"LLM_SYSTEM_PROMPT_FLAG\", True)\n",
    "tech_expert.set_modality_flag(\"AGENT_FLAG\", True)\n",
    "\n",
    "tech_expert_id = graphiti_framework.create_agent(\n",
    "    agent_config=tech_expert.get_config(),\n",
    "    agent_name=\"Technical Expert\",\n",
    "    description=\"Implementation-focused AI/ML expert\",\n",
    "    tags=[\"technical\", \"implementation\", \"coding\"]\n",
    ")\n",
    "\n",
    "# Create a Creative Strategist agent  \n",
    "creative_strategist = AgentConfig(\"creative_strategist_001\")\n",
    "creative_strategist.set_prompt(\"llmSystem\",\n",
    "    \"You are a Creative Strategist who thinks outside the box about AI applications. \"\n",
    "    \"You focus on innovative use cases, user experience, and strategic implications. \"\n",
    "    \"You bring creative perspectives to technical discussions.\")\n",
    "creative_strategist.set_modality_flag(\"LLM_SYSTEM_PROMPT_FLAG\", True)\n",
    "creative_strategist.set_modality_flag(\"AGENT_FLAG\", True)\n",
    "\n",
    "creative_strategist_id = graphiti_framework.create_agent(\n",
    "    agent_config=creative_strategist.get_config(),\n",
    "    agent_name=\"Creative Strategist\", \n",
    "    description=\"Innovation-focused strategy expert\",\n",
    "    tags=[\"creative\", \"strategy\", \"innovation\"]\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created agents:\")\n",
    "print(f\"   â€¢ Research Assistant: {agent_id}\")\n",
    "print(f\"   â€¢ Technical Expert: {tech_expert_id}\")\n",
    "print(f\"   â€¢ Creative Strategist: {creative_strategist_id}\")\n",
    "\n",
    "# Generate multi-agent conversation\n",
    "print(f\"\\nðŸŽ­ Generating multi-agent conversation...\")\n",
    "\n",
    "conversation = conversation_generator.generate_conversation(\n",
    "    agents=[agent_id, tech_expert_id, creative_strategist_id],\n",
    "    topic=\"Building Privacy-Preserving AI Systems with Local Models\",\n",
    "    num_turns=9,  # 3 turns per agent\n",
    "    context={\n",
    "        \"focus\": \"ollama_graphiti_integration\",\n",
    "        \"goal\": \"practical_implementation_guide\",\n",
    "        \"audience\": \"developers_and_researchers\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… Generated conversation: {conversation['conversation_id']}\")\n",
    "print(f\"ðŸ“ Topic: {conversation['topic']}\")\n",
    "print(f\"ðŸ‘¥ Participants: {len(conversation['participants'])} agents\")\n",
    "print(f\"ðŸ”„ Turns: {len(conversation['turns'])} exchanges\")\n",
    "\n",
    "# Show conversation preview\n",
    "print(f\"\\nðŸ’¬ Conversation Preview:\")\n",
    "for i, turn in enumerate(conversation['turns'][:6], 1):  # Show first 6 turns\n",
    "    agent_name = \"Research Assistant\" if turn['agent_id'] == agent_id else \\\n",
    "                 \"Technical Expert\" if turn['agent_id'] == tech_expert_id else \\\n",
    "                 \"Creative Strategist\"\n",
    "    \n",
    "    content_preview = turn['content'][:80] + \"...\" if len(turn['content']) > 80 else turn['content']\n",
    "    print(f\"   {i}. ðŸ¤– {agent_name}: {content_preview}\")\n",
    "\n",
    "# Export conversation to JSONL for training\n",
    "export_path = \"multi_agent_ollama_conversation.jsonl\"\n",
    "conversation_generator.export_conversation_jsonl(\n",
    "    conversation_id=conversation['conversation_id'],\n",
    "    output_path=export_path,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Exported to: {export_path}\")\n",
    "print(f\"ðŸŽ¯ Use case: Training data for multi-agent AI systems\")\n",
    "print(f\"ðŸ“ˆ Format: JSONL with agent personalities and Ollama integration context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Part 6: System Analytics and Data Export\n",
    "\n",
    "Let's analyze what we've built and explore the data export capabilities:\n",
    "- **Agent Statistics** - Review created agents and their configurations\n",
    "- **Knowledge Base Analysis** - Examine stored knowledge and embeddings\n",
    "- **Conversation Metrics** - Analyze multi-agent interaction patterns  \n",
    "- **Export Options** - Demonstrate various data export formats\n",
    "- **Performance Insights** - Show Polars database efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive system analytics and data export demonstration\n",
    "print(\"ðŸ“ˆ AMS-DB System Analytics Dashboard\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Agent Analytics\n",
    "print(\"\\nðŸ¤– Agent Statistics:\")\n",
    "agents_df = db_handler.agent_matrix\n",
    "print(f\"   â€¢ Total agents created: {agents_df.height}\")\n",
    "\n",
    "for row in agents_df.iter_rows(named=True):\n",
    "    agent_name = row[\"agent_name\"] \n",
    "    created_at = row[\"created_at\"]\n",
    "    config_size = len(row[\"config_json\"])\n",
    "    print(f\"   â€¢ {agent_name}: Created {created_at}, Config: {config_size:,} chars\")\n",
    "\n",
    "# 2. Knowledge Base Analytics\n",
    "print(f\"\\nðŸ“š Knowledge Base Statistics:\")\n",
    "knowledge_df = db_handler.knowledge_base\n",
    "print(f\"   â€¢ Total knowledge documents: {knowledge_df.height}\")\n",
    "\n",
    "if knowledge_df.height > 0:\n",
    "    # Group by agent\n",
    "    knowledge_by_agent = knowledge_df.group_by(\"agent_id\").agg([\n",
    "        knowledge_df.select(\"content\").count().alias(\"doc_count\"),\n",
    "        knowledge_df.select(\"content\").str.len_chars().mean().alias(\"avg_length\")\n",
    "    ])\n",
    "    \n",
    "    for row in knowledge_by_agent.iter_rows(named=True):\n",
    "        agent_id_short = row[\"agent_id\"][:20] + \"...\" if len(row[\"agent_id\"]) > 20 else row[\"agent_id\"]\n",
    "        print(f\"   â€¢ {agent_id_short}: {row['doc_count']} docs, {row['avg_length']:.0f} chars avg\")\n",
    "\n",
    "# 3. Conversation Analytics\n",
    "print(f\"\\nðŸ’¬ Conversation Statistics:\")\n",
    "conversations_df = db_handler.conversation_history\n",
    "print(f\"   â€¢ Total messages: {conversations_df.height}\")\n",
    "\n",
    "if conversations_df.height > 0:\n",
    "    # Messages by agent\n",
    "    messages_by_agent = conversations_df.group_by(\"agent_id\").agg([\n",
    "        conversations_df.select(\"content\").count().alias(\"message_count\")\n",
    "    ]).sort(\"message_count\", descending=True)\n",
    "    \n",
    "    for row in messages_by_agent.iter_rows(named=True):\n",
    "        agent_id_short = row[\"agent_id\"][:20] + \"...\" if len(row[\"agent_id\"]) > 20 else row[\"agent_id\"]\n",
    "        print(f\"   â€¢ {agent_id_short}: {row['message_count']} messages\")\n",
    "    \n",
    "    # Session analytics\n",
    "    unique_sessions = conversations_df.select(\"session_id\").n_unique()\n",
    "    print(f\"   â€¢ Unique sessions: {unique_sessions}\")\n",
    "\n",
    "# 4. Database Performance Metrics\n",
    "print(f\"\\nâš¡ Database Performance:\")\n",
    "db_stats = db_handler.get_database_stats()\n",
    "print(f\"   â€¢ Database size: {db_stats['database_size_mb']:.2f} MB\")\n",
    "print(f\"   â€¢ Query speed: Polars high-performance backend\")\n",
    "print(f\"   â€¢ Storage format: Parquet (columnar, compressed)\")\n",
    "\n",
    "# 5. Export Demonstrations\n",
    "print(f\"\\nðŸ“¤ Data Export Demonstrations:\")\n",
    "\n",
    "# Export agents as JSON for sharing configurations\n",
    "agents_export_path = \"ams_agents_export.json\"\n",
    "agents_data = []\n",
    "for row in agents_df.iter_rows(named=True):\n",
    "    agents_data.append({\n",
    "        \"agent_id\": row[\"agent_id\"],\n",
    "        \"agent_name\": row[\"agent_name\"], \n",
    "        \"description\": row[\"description\"],\n",
    "        \"created_at\": str(row[\"created_at\"]),\n",
    "        \"config\": json.loads(row[\"config_json\"])\n",
    "    })\n",
    "\n",
    "with open(agents_export_path, 'w') as f:\n",
    "    json.dump(agents_data, f, indent=2, default=str)\n",
    "print(f\"   âœ… Agents exported: {agents_export_path}\")\n",
    "\n",
    "# Export knowledge base as CSV for analysis\n",
    "if knowledge_df.height > 0:\n",
    "    knowledge_export_path = \"ams_knowledge_export.csv\"\n",
    "    knowledge_df.write_csv(knowledge_export_path)\n",
    "    print(f\"   âœ… Knowledge base exported: {knowledge_export_path}\")\n",
    "\n",
    "# Export conversations as Parquet for high-performance storage\n",
    "if conversations_df.height > 0:\n",
    "    conversations_export_path = \"ams_conversations_export.parquet\"\n",
    "    conversations_df.write_parquet(conversations_export_path)\n",
    "    print(f\"   âœ… Conversations exported: {conversations_export_path}\")\n",
    "\n",
    "# 6. Integration Summary\n",
    "print(f\"\\nðŸŽ¯ Integration Summary:\")\n",
    "print(f\"   âœ… Ollama Models: phi4:latest, gemma3:4b, nomic-embed-text\")\n",
    "print(f\"   âœ… Graphiti Integration: Knowledge graphs with temporal memory\")\n",
    "print(f\"   âœ… Polars Database: High-speed data operations\")\n",
    "print(f\"   âœ… Agent System: Complete configuration management\")\n",
    "print(f\"   âœ… Privacy: All processing local (no external API calls)\")\n",
    "print(f\"   âœ… Export Formats: JSON, CSV, Parquet, JSONL\")\n",
    "\n",
    "# 7. Next Steps for Users\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"   1. Experiment with different Ollama models\")\n",
    "print(f\"   2. Create domain-specific agent configurations\")\n",
    "print(f\"   3. Build larger knowledge bases for your use case\")\n",
    "print(f\"   4. Generate training datasets for model fine-tuning\")\n",
    "print(f\"   5. Integrate with your existing AI workflows\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ AMS-DB Ollama + Graphiti demonstration complete!\")\n",
    "print(f\"ðŸ“– This notebook showed the complete integration of:\")\n",
    "print(f\"   â€¢ Local LLM processing with Ollama\")\n",
    "print(f\"   â€¢ Temporal knowledge graphs with Graphiti\") \n",
    "print(f\"   â€¢ High-speed database operations with Polars\")\n",
    "print(f\"   â€¢ Comprehensive agent configuration management\")\n",
    "print(f\"   â€¢ Multi-agent conversation generation\")\n",
    "print(f\"   â€¢ Data export for ML workflows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a ShoeBot Sales Agent using LangGraph and Graphiti\n",
    "\n",
    "The following example demonstrates building an agent using LangGraph. Graphiti is used to personalize agent responses based on information learned from prior conversations. Additionally, a database of products is loaded into the Graphiti graph, enabling the agent to speak to these products.\n",
    "\n",
    "The agent implements:\n",
    "- persistence of new chat turns to Graphiti and recall of relevant Facts using the most recent message.\n",
    "- a tool for querying Graphiti for shoe information\n",
    "- an in-memory MemorySaver to maintain agent state.\n",
    "\n",
    "## Install dependencies\n",
    "```shell\n",
    "pip install graphiti-core langchain-openai langgraph ipywidgets\n",
    "```\n",
    "\n",
    "Ensure that you've followed the Graphiti installation instructions. In particular, installation of `neo4j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from contextlib import suppress\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Annotated\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith integration (Optional)\n",
    "\n",
    "If you'd like to trace your agent using LangSmith, ensure that you have a `LANGSMITH_API_KEY` set in your environment.\n",
    "\n",
    "Then set `os.environ['LANGCHAIN_TRACING_V2'] = 'false'` to `true`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'Graphiti LangGraph Tutorial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Graphiti\n",
    "\n",
    "Ensure that you have `neo4j` running and a database created. Ensure that you've configured the following in your environment.\n",
    "\n",
    "```bash\n",
    "NEO4J_URI=\n",
    "NEO4J_USER=\n",
    "NEO4J_PASSWORD=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Graphiti\n",
    "\n",
    "from graphiti_core import Graphiti\n",
    "from graphiti_core.edges import EntityEdge\n",
    "from graphiti_core.nodes import EpisodeType\n",
    "from graphiti_core.utils.maintenance.graph_data_operations import clear_data\n",
    "\n",
    "neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')\n",
    "neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')\n",
    "neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "\n",
    "client = Graphiti(\n",
    "    neo4j_uri,\n",
    "    neo4j_user,\n",
    "    neo4j_password,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a database schema \n",
    "\n",
    "The following is only required for the first run of this notebook or when you'd like to start your database over.\n",
    "\n",
    "**IMPORTANT**: `clear_data` is destructive and will wipe your entire database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will clear the database\n",
    "await clear_data(client.driver)\n",
    "await client.build_indices_and_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Shoe Data into the Graph\n",
    "\n",
    "Load several shoe and related products into the Graphiti. This may take a while.\n",
    "\n",
    "\n",
    "**IMPORTANT**: This only needs to be done once. If you run `clear_data` you'll need to rerun this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def ingest_products_data(client: Graphiti):\n",
    "    script_dir = Path.cwd().parent\n",
    "    json_file_path = script_dir / 'data' / 'manybirds_products.json'\n",
    "\n",
    "    with open(json_file_path) as file:\n",
    "        products = json.load(file)['products']\n",
    "\n",
    "    for i, product in enumerate(products):\n",
    "        await client.add_episode(\n",
    "            name=product.get('title', f'Product {i}'),\n",
    "            episode_body=str({k: v for k, v in product.items() if k != 'images'}),\n",
    "            source_description='ManyBirds products',\n",
    "            source=EpisodeType.json,\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "        )\n",
    "\n",
    "\n",
    "await ingest_products_data(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a user node in the Graphiti graph\n",
    "\n",
    "In your own app, this step could be done later once the user has identified themselves and made their sales intent known. We do this here so we can configure the agent with the user's `node_uuid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphiti_core.search.search_config_recipes import NODE_HYBRID_SEARCH_EPISODE_MENTIONS\n",
    "\n",
    "user_name = 'jess'\n",
    "\n",
    "await client.add_episode(\n",
    "    name='User Creation',\n",
    "    episode_body=(f'{user_name} is interested in buying a pair of shoes'),\n",
    "    source=EpisodeType.text,\n",
    "    reference_time=datetime.now(timezone.utc),\n",
    "    source_description='SalesBot',\n",
    ")\n",
    "\n",
    "# let's get Jess's node uuid\n",
    "nl = await client._search(user_name, NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
    "\n",
    "user_node_uuid = nl.nodes[0].uuid\n",
    "\n",
    "# and the ManyBirds node uuid\n",
    "nl = await client._search('ManyBirds', NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
    "manybirds_node_uuid = nl.nodes[0].uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_to_facts_string(entities: list[EntityEdge]):\n",
    "    return '-' + '\\n- '.join([edge.fact for edge in entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, add_messages\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_shoe_data` Tool\n",
    "\n",
    "The agent will use this to search the Graphiti graph for information about shoes. We center the search on the `manybirds_node_uuid` to ensure we rank shoe-related data over user data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def get_shoe_data(query: str) -> str:\n",
    "    \"\"\"Search the graphiti graph for information about shoes\"\"\"\n",
    "    edge_results = await client.search(\n",
    "        query,\n",
    "        center_node_uuid=manybirds_node_uuid,\n",
    "        num_results=10,\n",
    "    )\n",
    "    return edges_to_facts_string(edge_results)\n",
    "\n",
    "\n",
    "tools = [get_shoe_data]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4.1-mini', temperature=0).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tool node\n",
    "await tool_node.ainvoke({'messages': [await llm.ainvoke('wool shoes')]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Function Explanation\n",
    "\n",
    "The chatbot uses Graphiti to provide context-aware responses in a shoe sales scenario. Here's how it works:\n",
    "\n",
    "1. **Context Retrieval**: It searches the Graphiti graph for relevant information based on the latest message, using the user's node as the center point. This ensures that user-related facts are ranked higher than other information in the graph.\n",
    "\n",
    "2. **System Message**: It constructs a system message incorporating facts from Graphiti, setting the context for the AI's response.\n",
    "\n",
    "3. **Knowledge Persistence**: After generating a response, it asynchronously adds the interaction to the Graphiti graph, allowing future queries to reference this conversation.\n",
    "\n",
    "This approach enables the chatbot to maintain context across interactions and provide personalized responses based on the user's history and preferences stored in the Graphiti graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_name: str\n",
    "    user_node_uuid: str\n",
    "\n",
    "\n",
    "async def chatbot(state: State):\n",
    "    facts_string = None\n",
    "    if len(state['messages']) > 0:\n",
    "        last_message = state['messages'][-1]\n",
    "        graphiti_query = f'{\"SalesBot\" if isinstance(last_message, AIMessage) else state[\"user_name\"]}: {last_message.content}'\n",
    "        # search graphiti using Jess's node uuid as the center node\n",
    "        # graph edges (facts) further from the Jess node will be ranked lower\n",
    "        edge_results = await client.search(\n",
    "            graphiti_query, center_node_uuid=state['user_node_uuid'], num_results=5\n",
    "        )\n",
    "        facts_string = edges_to_facts_string(edge_results)\n",
    "\n",
    "    system_message = SystemMessage(\n",
    "        content=f\"\"\"You are a skillfull shoe salesperson working for ManyBirds. Review information about the user and their prior conversation below and respond accordingly.\n",
    "        Keep responses short and concise. And remember, always be selling (and helpful!)\n",
    "\n",
    "        Things you'll need to know about the user in order to close a sale:\n",
    "        - the user's shoe size\n",
    "        - any other shoe needs? maybe for wide feet?\n",
    "        - the user's preferred colors and styles\n",
    "        - their budget\n",
    "\n",
    "        Ensure that you ask the user for the above if you don't already know.\n",
    "\n",
    "        Facts about the user and their conversation:\n",
    "        {facts_string or 'No facts about the user and their conversation'}\"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [system_message] + state['messages']\n",
    "\n",
    "    response = await llm.ainvoke(messages)\n",
    "\n",
    "    # add the response to the graphiti graph.\n",
    "    # this will allow us to use the graphiti search later in the conversation\n",
    "    # we're doing async here to avoid blocking the graph execution\n",
    "    asyncio.create_task(\n",
    "        client.add_episode(\n",
    "            name='Chatbot Response',\n",
    "            episode_body=f'{state[\"user_name\"]}: {state[\"messages\"][-1]}\\nSalesBot: {response.content}',\n",
    "            source=EpisodeType.message,\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "            source_description='Chatbot',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Agent\n",
    "\n",
    "This section sets up the Agent's LangGraph graph:\n",
    "\n",
    "1. **Graph Structure**: It defines a graph with nodes for the agent (chatbot) and tools, connected in a loop.\n",
    "\n",
    "2. **Conditional Logic**: The `should_continue` function determines whether to end the graph execution or continue to the tools node based on the presence of tool calls.\n",
    "\n",
    "3. **Memory Management**: It uses a MemorySaver to maintain conversation state across turns. This is in addition to using Graphiti for facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "async def should_continue(state, config):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return 'end'\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return 'continue'\n",
    "\n",
    "\n",
    "graph_builder.add_node('agent', chatbot)\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "graph_builder.add_edge(START, 'agent')\n",
    "graph_builder.add_conditional_edges('agent', should_continue, {'continue': 'tools', 'end': END})\n",
    "graph_builder.add_edge('tools', 'agent')\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LangGraph agent graph is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(Exception):\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "Let's test the agent with a single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.ainvoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'What sizes do the TinyBirds Wool Runners in Natural Black come in?',\n",
    "            }\n",
    "        ],\n",
    "        'user_name': user_name,\n",
    "        'user_node_uuid': user_node_uuid,\n",
    "    },\n",
    "    config={'configurable': {'thread_id': uuid.uuid4().hex}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Graph\n",
    "\n",
    "At this stage, the graph would look something like this. The `jess` node is `INTERESTED_IN` the `TinyBirds Wool Runner` node. The image below was generated using Neo4j Desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='tinybirds-jess.png', width=850))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent interactively\n",
    "\n",
    "The following code will run the agent in an event loop. Just enter a message into the box and click submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_output = widgets.Output()\n",
    "config = {'configurable': {'thread_id': uuid.uuid4().hex}}\n",
    "user_state = {'user_name': user_name, 'user_node_uuid': user_node_uuid}\n",
    "\n",
    "\n",
    "async def process_input(user_state: State, user_input: str):\n",
    "    conversation_output.append_stdout(f'\\nUser: {user_input}\\n')\n",
    "    conversation_output.append_stdout('\\nAssistant: ')\n",
    "\n",
    "    graph_state = {\n",
    "        'messages': [{'role': 'user', 'content': user_input}],\n",
    "        'user_name': user_state['user_name'],\n",
    "        'user_node_uuid': user_state['user_node_uuid'],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async for event in graph.astream(\n",
    "            graph_state,\n",
    "            config=config,\n",
    "        ):\n",
    "            for value in event.values():\n",
    "                if 'messages' in value:\n",
    "                    last_message = value['messages'][-1]\n",
    "                    if isinstance(last_message, AIMessage) and isinstance(\n",
    "                        last_message.content, str\n",
    "                    ):\n",
    "                        conversation_output.append_stdout(last_message.content)\n",
    "    except Exception as e:\n",
    "        conversation_output.append_stdout(f'Error: {e}')\n",
    "\n",
    "\n",
    "def on_submit(b):\n",
    "    user_input = input_box.value\n",
    "    input_box.value = ''\n",
    "    asyncio.create_task(process_input(user_state, user_input))\n",
    "\n",
    "\n",
    "input_box = widgets.Text(placeholder='Type your message here...')\n",
    "submit_button = widgets.Button(description='Send')\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "conversation_output.append_stdout('Asssistant: Hello, how can I help you find shoes today?')\n",
    "\n",
    "display(widgets.VBox([input_box, submit_button, conversation_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
